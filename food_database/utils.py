# AUTOGENERATED! DO NOT EDIT! File to edit: ../notebooks/99-utils.ipynb.

# %% auto 0
__all__ = ['root', 'mt', 'md', 'stop_words', 'lemmatizer', 'nlp', 'unit_tagger', 'fractions', 'detokenize',
           'clean_ingredient_string', 'contains_whole_word', 'count_list_matches', 'train_unit_tagger',
           'join_repeated_numeric_tags', 'convert_fractions_to_decimal', 'tag_numerics_with_float_value',
           'get_unit_type', 'tag_units', 'sizeof_fmt', 'show_var_sizes', 'clear_variable_cache']

# %% ../notebooks/99-utils.ipynb 2
import sys

from pyprojroot import here
root = here()

import pandas as pd
import numpy as np

from nltk.corpus import stopwords
from sacremoses import MosesTokenizer, MosesDetokenizer
mt, md = MosesTokenizer(lang='en'), MosesDetokenizer(lang='en')
from nltk.stem import WordNetLemmatizer

import spacy
from spacy.matcher import Matcher
from spacy.util import filter_spans

import json
from itertools import groupby
import re
import string

from tqdm import tqdm
tqdm.pandas()

from fractions import Fraction
from word2number import w2n
import logging

# %% ../notebooks/99-utils.ipynb 4
with open(f'{root}/data/globals/unit_conversions.json') as f:
    unit_list = json.load(f)

# %% ../notebooks/99-utils.ipynb 6
stop_words = set(stopwords.words('english'))
stop_words.remove('can')

lemmatizer = WordNetLemmatizer()

# %% ../notebooks/99-utils.ipynb 7
def detokenize(tokens):
    text = md.detokenize(tokens)
    text = re.sub(r'\s/\s', '/', text)
    return text

# %% ../notebooks/99-utils.ipynb 8
def clean_ingredient_string(ingredient):
    if pd.isnull(ingredient) or not ingredient: return ingredient
    ingredient = str(ingredient).lower()
    ingredient = re.sub(r'^\W+', '', ingredient) # remove poorly parsed punctuation
    ingredient = re.sub(r'[(),.]', '', ingredient) # comments often bracketted, which should be removed
    ingredient = ingredient.strip()
    tokens = mt.tokenize(ingredient)
    tokens = [token for token in tokens if token not in stop_words]
    tokens = [lemmatizer.lemmatize(token) for token in tokens]
    return detokenize(tokens)

# %% ../notebooks/99-utils.ipynb 11
def contains_whole_word(string, search_word):
    if re.search(r"\b" + re.escape(search_word) + r"\b", string):
        return True
    else: 
        return False
    
def count_list_matches(list, search_strings):
    match_count = 0
    for item in list:
        for search_string in search_strings:
            if contains_whole_word(item, search_string):
                match_count += 1
    return match_count

# %% ../notebooks/99-utils.ipynb 13
def train_unit_tagger(matcher):

    for unit_type in unit_list.keys():
        for unit in unit_list[unit_type].keys():
            match_strings = [unit.replace('_',' ')] + unit_list[unit_type][unit]['matches']
            filters = [[{'LOWER': match_word} for match_word in match_string.split(" ")] for match_string in match_strings]
            matcher.add(unit, filters)

    filters = [[{"POS": "NUM"}]]
    matcher.add("numeric", filters)

    return matcher

nlp = spacy.load("en_core_web_sm")
unit_tagger = Matcher(nlp.vocab)
unit_tagger = train_unit_tagger(unit_tagger)

# %% ../notebooks/99-utils.ipynb 18
# numeric tags which come consecitvely should be read as a single tag eg. 1 1/2.
def join_repeated_numeric_tags(matches):
    numeric_repeats = [ (x[3] == 'numeric' and x[3] == y[3] and x[2] == y[1]) for x, y in zip(matches, matches[1:]) ] + [ False ] # finding consecutive numeric repeat values
    new_matches = []
    for i, match in enumerate(matches):
        if numeric_repeats[i-1]: continue # skip if previous was a numeric, current index would've been added to previous element
        while numeric_repeats[i]: i += 1 # need count in case of multiple consecitve numerics
        new_matches.append((match[0], match[1], matches[i][2], match[3]))

    return new_matches

# %% ../notebooks/99-utils.ipynb 20
fractions = ['1/2', '1/3', '2/3', '1/4', '3/4', '1/5', '2/5', '3/5', '4/5']

# %% ../notebooks/99-utils.ipynb 22
def convert_fractions_to_decimal(numeric_string):
    final_number = 0.0
    for string_num in numeric_string.split(" "):
        if '-' in string_num:
            string_num = string_num.split('-')[0] # range of numbers (eg. 4-6 onions)
        if '/' in string_num:
            try:
                split = string_num.split('/')
                if (float(split[0])/float(split[1])) < 0.75: # if fraction is greater than 3/4 then it isn't a fraction - it's an either/or measure (eg. 7/8 onions)
                    string_num = str(float(Fraction(string_num)))                
                else:
                    string_num = split[0]
            except (ValueError):
                pass
        try:
            final_number += float(string_num)
        except ValueError:
            try:
                final_number += w2n.word_to_num(string_num) # number word (eg. one onion)
            except ValueError:
                pass
    return str(final_number)

# %% ../notebooks/99-utils.ipynb 24
# we want to tag the units as their actual values
def tag_numerics_with_float_value(matches):
    [ (match[0], match[1], match[2], convert_fractions_to_decimal(match[3])) for match in matches ]

# %% ../notebooks/99-utils.ipynb 25
def get_unit_type(unit_tags):
    for unit_type in unit_list.keys():
        if any([t in unit_list[unit_type] for t in unit_tags]):
            return unit_type
    return 'portion'

# %% ../notebooks/99-utils.ipynb 26
def tag_units(phrase):

    if pd.isnull(phrase): return ([], [], 'portion')

    doc = nlp(phrase)
    matches = unit_tagger(doc)

    # overwriting duplicate tags by largest size
    spans = [doc[start:end] for match_id, start, end in matches]
    filtered_idxs = [(span.start, span.end) for span in filter_spans(spans)]
    matches = [match for match in matches if (match[1], match[2]) in filtered_idxs]

    # update with tag names
    matches = [ match + (nlp.vocab.strings[match[0]],) for match in matches ]

    matches = join_repeated_numeric_tags(matches)
    matches = [ (match[0], match[1], match[2], convert_fractions_to_decimal(str(doc[match[1]:match[2]]))) if match[3] == 'numeric' else match for match in matches ]

    # remaining non-tagged tokens
    match_idxs = [range(start,end) for match_id, start, end, tag in matches]
    match_idxs_flattened = [element for sublist in match_idxs for element in sublist]
    remainders = [ str(doc[i]) for i in range(0,len(doc)) if i not in match_idxs_flattened ]

    # removing punctuation from strings 
    remainders = [ word.translate(str.maketrans('', '', string.punctuation)) for word in remainders ]
    remainders = [ word.strip() for word in remainders ]
    remainders = list(filter(None, remainders))

    unit_tags = list(list(zip(*matches))[3]) if matches else []
    unit_type = get_unit_type(unit_tags)

    return unit_tags, remainders, unit_type

# %% ../notebooks/99-utils.ipynb 31
import sys
def sizeof_fmt(num, suffix='B'):
    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''
    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:
        if abs(num) < 1024.0:
            return "%3.1f %s%s" % (num, unit, suffix)
        num /= 1024.0
    return "%.1f %s%s" % (num, 'Yi', suffix)

def show_var_sizes():
    for name, size in sorted(((name, sys.getsizeof(value)) for name, value in list(
                            locals().items())), key= lambda x: -x[1])[:10]:
        print("{:>30}: {:>8}".format(name, sizeof_fmt(size)))

# %% ../notebooks/99-utils.ipynb 32
def clear_variable_cache():
    count = 0
    for name in dir():
        if re.search(r'^_[a-z]?[0-9]+', name) or re.search(r'^_+$', name):
            count += 1
            globals()[name] = None
    return count
