{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils.parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pyprojroot import here\n",
    "root = here()\n",
    "import sys\n",
    "sys.path.append(str(root))\n",
    "\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "import logging\n",
    "import warnings\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from food_database.logger import *\n",
    "\n",
    "import math\n",
    "\n",
    "import logging\n",
    "logger = configure_logger(logging.getLogger(__name__))\n",
    "\n",
    "def convert_size(size_bytes):\n",
    "   if size_bytes == 0:\n",
    "       return \"0B\"\n",
    "   size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n",
    "   i = int(math.floor(math.log(size_bytes, 1024)))\n",
    "   p = math.pow(1024, i)\n",
    "   s = round(size_bytes / p, 2)\n",
    "   return \"%s %s\" % (s, size_name[i])\n",
    "\n",
    "def parallel_apply(df, func, meta, npartitions=int(0.9 * mp.cpu_count()), keep_index=False, **kargs):\n",
    "    \n",
    "    # resetting MultiIndex as not supported in Dask (applied back later)\n",
    "    og_index = df.index\n",
    "    df.reset_index(inplace=True, drop=(not keep_index))\n",
    "    ddf = dd.from_pandas(df, npartitions=npartitions)\n",
    "\n",
    "    memory_usage = ddf.memory_usage(deep=True, index=False).compute()\n",
    "    if isinstance(memory_usage, pd.Series): memory_usage = memory_usage.sum()\n",
    "\n",
    "    logger.info(f\"Commencing parallel apply\")\n",
    "    logger.info(f\"DF shape: {df.shape} | DF size: {convert_size(memory_usage)}\")\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        with LocalCluster(n_workers=int(0.9 * mp.cpu_count()),\n",
    "            processes=True,\n",
    "            threads_per_worker=1,\n",
    "            memory_limit='1.5GB',\n",
    "        ) as cluster, Client(cluster) as client:\n",
    "            results = ddf.apply(func, meta=meta, axis=1, **kargs).compute() if isinstance(df, pd.DataFrame) else ddf.apply(func, meta=meta, **kargs).compute()\n",
    "\n",
    "    df.index = og_index\n",
    "    results.index = og_index\n",
    "\n",
    "    return results\n",
    "\n",
    "def chunk_df(df, n=1e6):\n",
    "    logger.info(f'Splitting dataframe into {df.shape[0]/n} chunks of size {n}')\n",
    "    return [df[i:i+int(n)] for i in range(0,df.shape[0],int(n))]\n",
    "\n",
    "def initialize_chunk_dir(save_path):\n",
    "    save_path = Path(save_path)\n",
    "    file_name = save_path.name\n",
    "    if '.' in file_name: file_name = file_name.split('.')[0]\n",
    "    chunk_dir = Path(save_path.parent)/f'{file_name}_chunks'\n",
    "    chunk_dir.mkdir(exist_ok=True)\n",
    "    return chunk_dir\n",
    "\n",
    "def parallel_apply_chunks(df, func, meta, chunksize, save_path, npartitions=int(0.9 * mp.cpu_count()), keep_index=False, start_from=0, **kargs):\n",
    "\n",
    "    chunk_dir = initialize_chunk_dir(save_path)\n",
    "    chunks = chunk_df(df, chunksize)\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "\n",
    "        if i < start_from: continue\n",
    "        if (chunk_dir/f\"{i}.feather\").exists(): continue\n",
    "\n",
    "        logger.info(f'------------------')\n",
    "        logger.info(f'COMMENCING CHUNK {i}')\n",
    "        logger.info(f'------------------')\n",
    "\n",
    "        result = parallel_apply(chunk, func, meta, npartitions, keep_index, **kargs)\n",
    "        if isinstance(result, pd.Series): result = result.to_frame()\n",
    "        result.to_feather(chunk_dir/f'{i}.feather')\n",
    "\n",
    "    compiled_results = compile_chunks(save_path)\n",
    "\n",
    "    return compiled_results\n",
    "\n",
    "def compile_chunks(save_path):\n",
    "\n",
    "    logger.info(\"Compiling chunks\")\n",
    "\n",
    "    save_path = Path(save_path)\n",
    "    chunk_dir = initialize_chunk_dir(save_path)\n",
    "\n",
    "    compiled_chunks = pd.DataFrame()\n",
    "    for i in range(len(list(chunk_dir.iterdir()))):\n",
    "        chunk = pd.read_feather(chunk_dir/f\"{i}.feather\")\n",
    "        compiled_chunks = pd.concat([compiled_chunks, chunk], axis=0)\n",
    "\n",
    "    compiled_chunks.to_feather(save_path)\n",
    "\n",
    "    if compiled_chunks.shape[1] == 1: # convert to series if series\n",
    "        compiled_chunks = compiled_chunks.iloc[:, 0]\n",
    "\n",
    "    return compiled_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev import nbdev_export; nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recipes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
