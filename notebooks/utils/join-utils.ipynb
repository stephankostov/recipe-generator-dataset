{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "{}\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp utils.join_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from pyprojroot import here\n",
    "root = here()\n",
    "import sys\n",
    "sys.path.append(str(root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from nltk.corpus import wordnet\n",
    "import warnings\n",
    "from food_database.utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ingredients_df = pd.read_feather(f'{root}/data/local/recipe/partial/ingredients/0.feather')\n",
    "ingredients = list(ingredients_df.iloc[:10]['name.name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def get_synset(ingredient):\n",
    "\n",
    "    synsets = wordnet.synsets(ingredient)\n",
    "    if not synsets: return None\n",
    "\n",
    "    filtered = [w for w in synsets if 'food' in w.lexname()]\n",
    "    if filtered: synsets = filtered\n",
    "    filtered = [w for w in synsets if ingredient in w.name()]\n",
    "    if filtered: synsets = filtered\n",
    "\n",
    "    return synsets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We return indexes here to select the right synonyms. The synonyms method seems to use the synset, but uses a simplified API without going through the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some words are causing issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "excluded_words = [\n",
    "    'cut'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_word(word):\n",
    "    tokens = mt.tokenize(word)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return detokenize(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_food_hypernyms(synset):\n",
    "\n",
    "    with warnings.catch_warnings(): # closure throws warning if it exceeds depth limit\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        hypernyms = list(synset.closure(lambda x: x.hypernyms(), depth=5))\n",
    "\n",
    "    hypernyms = [ word.name().split('.')[0] for word in hypernyms ]\n",
    "    \n",
    "    try:\n",
    "        hypernyms = hypernyms[:(hypernyms.index('food'))] \n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    hypernyms = hypernyms[:7]\n",
    "    \n",
    "    return hypernyms\n",
    "\n",
    "def flatten_list(l):\n",
    "    return [x for xs in l for x in xs]\n",
    "\n",
    "def clean_word(word):\n",
    "    tokens = mt.tokenize(word)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return detokenize(tokens)\n",
    "\n",
    "def clean_alt_words(alt_words):\n",
    "    alt_words = [reversed(w.split('_')) for w in alt_words]\n",
    "    alt_words = flatten_list(alt_words)\n",
    "    alt_words = [clean_word(w) for w in alt_words]\n",
    "    alt_words = [w for w in alt_words if w not in excluded_words]\n",
    "    alt_words = list(filter(None, alt_words))\n",
    "    return alt_words\n",
    "\n",
    "def find_alt_words(word):\n",
    "\n",
    "    if not isinstance(word, str) or word == '': return \n",
    "\n",
    "    synset = get_synset(word)\n",
    "    if not synset: return\n",
    "       \n",
    "    synonyms = list(set(synset.lemma_names()) - {word})\n",
    "    hypernyms = get_food_hypernyms(synset)\n",
    "\n",
    "    alt_words = [ *synonyms, *hypernyms ]\n",
    "    \n",
    "    alt_words = clean_alt_words(alt_words)\n",
    "\n",
    "    alt_words = alt_words[:10]\n",
    "\n",
    "    return alt_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'eggplant' in find_alt_words('aubergine')\n",
    "assert 'bread' in find_alt_words('baguette')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pork', 'meat']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_alt_words('bacon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['courgette'] ['summer_squash', 'squash', 'vegetable', 'produce']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['courgette', 'squash', 'summer', 'squash', 'vegetable', 'produce']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_alt_words('zucchini')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a particular problem here, in that sometimes phrases are returned ('cut of pork'). What can we do about this? We could separate the words, however they only hold their meaning when they are together eg. baked_goods. We don't necessarily want the 'goods' doesn't mean anything really, and we don't necessarily want to word baked in there without it. This actually only stands if there is only one search term here. Likely these words will be refining terms rather than a search solely on this, in which case it should work. \n",
    "\n",
    "Lets do what we have done with our standard ingredient strings: separate them and reorder them so the last noun comes first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steph/.conda/envs/recipes/lib/python3.10/site-packages/nbdev/export.py:73: UserWarning: Notebook '/home/steph/workspace/food-database/06-density-db-final.ipynb' uses `#|export` without `#|default_exp` cell.\n",
      "Note nbdev2 no longer supports nbdev1 syntax. Run `nbdev_migrate` to upgrade.\n",
      "See https://nbdev.fast.ai/getting_started.html for more information.\n",
      "  warn(f\"Notebook '{nbname}' uses `#|export` without `#|default_exp` cell.\\n\"\n",
      "/home/steph/.conda/envs/recipes/lib/python3.10/site-packages/nbdev/export.py:73: UserWarning: Notebook '/home/steph/workspace/food-database/09-molecule-db-finalising.ipynb' uses `#|export` without `#|default_exp` cell.\n",
      "Note nbdev2 no longer supports nbdev1 syntax. Run `nbdev_migrate` to upgrade.\n",
      "See https://nbdev.fast.ai/getting_started.html for more information.\n",
      "  warn(f\"Notebook '{nbname}' uses `#|export` without `#|default_exp` cell.\\n\"\n"
     ]
    }
   ],
   "source": [
    "from nbdev import nbdev_export; nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recipes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
