{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: utils.html\n",
    "title: Logging\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import sys\n",
    "\n",
    "from pyprojroot import here\n",
    "root = here()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sacremoses import MosesTokenizer, MosesDetokenizer\n",
    "mt, md = MosesTokenizer(lang='en'), MosesDetokenizer(lang='en')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "import json\n",
    "from itertools import groupby\n",
    "import re\n",
    "import string\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from fractions import Fraction\n",
    "from word2number import w2n\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to copy all utils functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "with open(f'{root}/data/globals/unit_conversions.json') as f:\n",
    "    unit_list = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingredient Cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.remove('can')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def detokenize(tokens):\n",
    "    text = md.detokenize(tokens)\n",
    "    text = re.sub(r'\\s/\\s', '/', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def clean_ingredient_string(ingredient):\n",
    "    if pd.isnull(ingredient) or not ingredient: return ingredient\n",
    "    ingredient = str(ingredient).lower()\n",
    "    ingredient = re.sub(r'^\\W+', '', ingredient) # remove poorly parsed punctuation\n",
    "    ingredient = re.sub(r'[(),.]', '', ingredient) # comments often bracketted, which should be removed\n",
    "    ingredient = ingredient.strip()\n",
    "    tokens = mt.tokenize(ingredient)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return detokenize(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert clean_ingredient_string('pecan halves') == clean_ingredient_string('pecan') + ' ' + clean_ingredient_string('halves')\n",
    "assert clean_ingredient_string('raspberries') == 'raspberry'\n",
    "assert clean_ingredient_string('') == ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DF Text Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def contains_whole_word(string, search_word):\n",
    "    if re.search(r\"\\b\" + re.escape(search_word) + r\"\\b\", string):\n",
    "        return True\n",
    "    else: \n",
    "        return False\n",
    "    \n",
    "def count_list_matches(list, search_strings):\n",
    "    match_count = 0\n",
    "    for item in list:\n",
    "        for search_string in search_strings:\n",
    "            if contains_whole_word(item, search_string):\n",
    "                match_count += 1\n",
    "    return match_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train_unit_tagger(matcher):\n",
    "\n",
    "    for unit_type in unit_list.keys():\n",
    "        for unit in unit_list[unit_type].keys():\n",
    "            match_strings = [unit.replace('_',' ')] + unit_list[unit_type][unit]['matches']\n",
    "            filters = [[{'LOWER': match_word} for match_word in match_string.split(\" \")] for match_string in match_strings]\n",
    "            matcher.add(unit, filters)\n",
    "\n",
    "    filters = [[{\"POS\": \"NUM\"}]]\n",
    "    matcher.add(\"numeric\", filters)\n",
    "\n",
    "    return matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "unit_tagger = Matcher(nlp.vocab)\n",
    "unit_tagger = train_unit_tagger(unit_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15619426672489268670 numeric 2 3 2\n",
      "15619426672489268670 numeric 3 4 1/2\n",
      "4504748935369724861 tablespoon 4 5 tbsp\n",
      "15212922390252438877 ounce 6 7 oz\n",
      "15619426672489268670 numeric 9 10 500\n",
      "5673684224853602733 milliliter 10 11 ml\n",
      "8861518500936608369 pint 15 16 pint\n",
      "3579143477707580839 fluid_ounce 19 21 fl oz\n",
      "15619426672489268670 numeric 22 23 1\n",
      "3550475366760073739 gram 23 24 g\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I need 2 1/2 tbsp of oz sugar and 500 ml of milk and a pint what about a fl oz and 1 g of salt\")\n",
    "matches = unit_tagger(doc)\n",
    "\n",
    "# overwriting duplicate tags by largest size\n",
    "spans = [doc[start:end] for match_id, start, end in matches]\n",
    "filtered_idxs = [(span.start, span.end) for span in filter_spans(spans)]\n",
    "matches = [match for match in matches if (match[1], match[2]) in filtered_idxs]\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with various spellings for units, we could tag them with our NLP model to keep consistency.\n",
    "\n",
    "What we want to do here is have a function which tags a string with the 'base' spellings of the units. How should this be outputted?\n",
    "\n",
    "- The string with the tagged 'base' spelling replacing the words\n",
    "- Lists: One for the tags and another for the remainder\n",
    "\n",
    "How we want to output this depends on what we are going to do with the outputs. What are we going to do? \n",
    "The unit doesn't actually matter that much. What matters more is the type of food that we want, which is moreso found in the description/comment of the food. The unit search matters most when it's not actually a standard unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import zip_longest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. check for match indices\n",
    "2. go through an correct each match index\n",
    "3. remove values with previous matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# numeric tags which come consecitvely should be read as a single tag eg. 1 1/2.\n",
    "def join_repeated_numeric_tags(matches):\n",
    "    numeric_repeats = [ (x[3] == 'numeric' and x[3] == y[3] and x[2] == y[1]) for x, y in zip(matches, matches[1:]) ] + [ False ] # finding consecutive numeric repeat values\n",
    "    new_matches = []\n",
    "    for i, match in enumerate(matches):\n",
    "        if numeric_repeats[i-1]: continue # skip if previous was a numeric, current index would've been added to previous element\n",
    "        while numeric_repeats[i]: i += 1 # need count in case of multiple consecitve numerics\n",
    "        new_matches.append((match[0], match[1], matches[i][2], match[3]))\n",
    "\n",
    "    return new_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'one'.isnumeric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "fractions = ['1/2', '1/3', '2/3', '1/4', '3/4', '1/5', '2/5', '3/5', '4/5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def convert_fractions_to_decimal(numeric_string):\n",
    "    final_number = 0.0\n",
    "    for string_num in numeric_string.split(\" \"):\n",
    "        if '-' in string_num:\n",
    "            string_num = string_num.split('-')[0] # range of numbers (eg. 4-6 onions)\n",
    "        if '/' in string_num:\n",
    "            try:\n",
    "                split = string_num.split('/')\n",
    "                if (float(split[0])/float(split[1])) < 0.75: # if fraction is greater than 3/4 then it isn't a fraction - it's an either/or measure (eg. 7/8 onions)\n",
    "                    string_num = str(float(Fraction(string_num)))                \n",
    "                else:\n",
    "                    string_num = split[0]\n",
    "            except (ValueError):\n",
    "                pass\n",
    "        try:\n",
    "            final_number += float(string_num)\n",
    "        except ValueError:\n",
    "            try:\n",
    "                final_number += w2n.word_to_num(string_num) # number word (eg. one onion)\n",
    "            except ValueError:\n",
    "                pass\n",
    "    return str(final_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert convert_fractions_to_decimal('one') == '1.0'\n",
    "assert convert_fractions_to_decimal('1 1/2') == '1.5'\n",
    "assert convert_fractions_to_decimal('4-6') == '4.0'\n",
    "assert convert_fractions_to_decimal('7/8') == '7.0'\n",
    "assert convert_fractions_to_decimal('pie') == '0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# we want to tag the units as their actual values\n",
    "def tag_numerics_with_float_value(matches):\n",
    "    [ (match[0], match[1], match[2], convert_fractions_to_decimal(match[3])) for match in matches ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_unit_type(unit_tags):\n",
    "    for unit_type in unit_list.keys():\n",
    "        if any([t in unit_list[unit_type] for t in unit_tags]):\n",
    "            return unit_type\n",
    "    return 'portion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def tag_units(phrase):\n",
    "\n",
    "    if pd.isnull(phrase): return ([], [], 'portion')\n",
    "\n",
    "    doc = nlp(phrase)\n",
    "    matches = unit_tagger(doc)\n",
    "\n",
    "    # overwriting duplicate tags by largest size\n",
    "    spans = [doc[start:end] for match_id, start, end in matches]\n",
    "    filtered_idxs = [(span.start, span.end) for span in filter_spans(spans)]\n",
    "    matches = [match for match in matches if (match[1], match[2]) in filtered_idxs]\n",
    "\n",
    "    # update with tag names\n",
    "    matches = [ match + (nlp.vocab.strings[match[0]],) for match in matches ]\n",
    "\n",
    "    matches = join_repeated_numeric_tags(matches)\n",
    "    matches = [ (match[0], match[1], match[2], convert_fractions_to_decimal(str(doc[match[1]:match[2]]))) if match[3] == 'numeric' else match for match in matches ]\n",
    "\n",
    "    # remaining non-tagged tokens\n",
    "    match_idxs = [range(start,end) for match_id, start, end, tag in matches]\n",
    "    match_idxs_flattened = [element for sublist in match_idxs for element in sublist]\n",
    "    remainders = [ str(doc[i]) for i in range(0,len(doc)) if i not in match_idxs_flattened ]\n",
    "\n",
    "    # removing punctuation from strings \n",
    "    remainders = [ word.translate(str.maketrans('', '', string.punctuation)) for word in remainders ]\n",
    "    remainders = [ word.strip() for word in remainders ]\n",
    "    remainders = list(filter(None, remainders))\n",
    "\n",
    "    unit_tags = list(list(zip(*matches))[3]) if matches else []\n",
    "    unit_type = get_unit_type(unit_tags)\n",
    "\n",
    "    return unit_tags, remainders, unit_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['1.0'], [], 'portion')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_units('one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tag_units('portion of 1 1/2 or 3 1/2 5')[0] == (['portion', '1.5', '8.5'])\n",
    "assert tag_units('serving 1/2 cup')[0] == ['portion', '0.5', 'cup']\n",
    "assert tag_units('cup fl oz chicken stock') == (['cup', 'fluid_ounce'], ['chicken', 'stock'], 'volume')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['portion', '0.5', 'cup'], [], 'volume')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_units('serving 1/2 cup')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import sys\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "def show_var_sizes():\n",
    "    for name, size in sorted(((name, sys.getsizeof(value)) for name, value in list(\n",
    "                            locals().items())), key= lambda x: -x[1])[:10]:\n",
    "        print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def clear_variable_cache():\n",
    "    count = 0\n",
    "    for name in dir():\n",
    "        if re.search(r'^_[a-z]?[0-9]+', name) or re.search(r'^_+$', name):\n",
    "            count += 1\n",
    "            globals()[name] = None\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev import nbdev_export; nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
