# AUTOGENERATED! DO NOT EDIT! File to edit: ../../notebooks/01-recipes-db-process.ipynb.

# %% auto 0
__all__ = ['root', 'mt', 'md', 'tag_map', 'filters', 'preprocess_cup_units', 'tag_ingredient_string', 'preprocess_remove_ors',
           'ner_preprocess_ingredient_string', 'parse_ingredient_string', 'pipeline_parse_ingredient_string_parsing',
           'is_number', 'clean_quantity', 'find_ner_match', 'tokenize_with_spans', 'get_match_idxs', 'split_nouns',
           'split_ingredient_fields_by_noun', 'remove_name_from_description', 'filter_patterns']

# %% ../../notebooks/01-recipes-db-process.ipynb 7
from pyprojroot import here
root = here()
import sys
sys.path.append(str(root))

# %% ../../notebooks/01-recipes-db-process.ipynb 8
import pandas as pd
import numpy as np

import nltk
import spacy
from spacy.matcher import Matcher
from spacy.util import filter_spans

import json
from itertools import groupby
import re
import string
import time

from ast import literal_eval

from tqdm import tqdm
tqdm.pandas()

from ..utils.utils import *

from parse_ingredients import parse_ingredient as parse_ingredient_rgx
from ingredient_parser import parse_ingredient as parse_ingredient_nlp

import dask.dataframe as dd
from dask.distributed import Client, LocalCluster
import multiprocessing as mp

from ..utils.parallel import *

from pathlib import Path

from sacremoses import MosesTokenizer, MosesDetokenizer
mt, md = MosesTokenizer(lang='en'), MosesDetokenizer(lang='en')

# %% ../../notebooks/01-recipes-db-process.ipynb 10
with open(f'{root}/config/unit_conversions.json') as f:
    unit_list = json.load(f)

# %% ../../notebooks/01-recipes-db-process.ipynb 59
def preprocess_cup_units(ingredient_string):
    ingredient_string = re.sub(r'\bc\.', 'cup', ingredient_string)
    return ingredient_string

# %% ../../notebooks/01-recipes-db-process.ipynb 72
tag_map = {'NN': 'noun', 'VB': 'verb', 'JJ': 'adj', 'RB': 'adv'}
def tag_ingredient_string(ingredient_string, is_item=True):
    tagged_words = nltk.pos_tag(nltk.word_tokenize(ingredient_string))
    mapped_words = [{'word': w[0], 'tag': tag_map[w[1][:2]] if w[1][:2] in tag_map else 'nan'} for w in tagged_words]
    if is_item and mapped_words:
        mapped_words[-1]['tag'] = 'noun' # making sure last word is noun, as these are items that we are labelling
    return mapped_words

# %% ../../notebooks/01-recipes-db-process.ipynb 75
def preprocess_remove_ors(ingredient_string):
    
    or_search = re.search(r'\bor\b', ingredient_string)
    if not or_search: return ingredient_string

    prev_string = ingredient_string[:or_search.span()[0]]
    following_string = ingredient_string[or_search.span()[-1]:]

    prev_tokens = tag_ingredient_string(prev_string, is_item=False)
    following_tokens = tag_ingredient_string(following_string, is_item=False)

    prev_nouns = any([token for token in prev_tokens if token['tag'] == 'noun'])

    if re.search(r'\(\s?or.*\)', ingredient_string):
        ingredient_string = re.sub(r'\(\s?or.*\)', '', ingredient_string)
    elif re.search(r'(\bor\b).*\d', ingredient_string) and prev_nouns:
        ingredient_string = re.sub(r'(\bor\b).*', '', ingredient_string)
    else:
        if not following_tokens: 
            ingredient_string = prev_string
        else:
            i = 0
            while i < len(following_tokens) and not following_tokens[i]['tag'] == 'noun': i += 1
            if i < len(following_tokens) and following_tokens[i]['tag'] == 'noun':
                if i < len(following_tokens)-1: pass
                if i == 0:
                    following_tokens = []
                else:
                    following_tokens = following_tokens[i:]
            else:
                following_tokens = []
            ingredient_words = prev_string.split(' ') + [word['word'] for word in following_tokens]
            ingredient_string = detokenize(sorted(set(ingredient_words), key=ingredient_words.index))
    
    # postprocess
    ingredient_string = ingredient_string.strip()
    ingredient_string = re.sub(r'\s\s+', ' ', ingredient_string) # double spaces
    ingredient_string = re.sub(r'[,\(.]$', '', ingredient_string) # trailing punctuation

    return ingredient_string

# %% ../../notebooks/01-recipes-db-process.ipynb 81
def ner_preprocess_ingredient_string(ingredient_string):
    ingredient_string = preprocess_cup_units(ingredient_string)
    ingredient_string = preprocess_remove_ors(ingredient_string)
    return ingredient_string

# %% ../../notebooks/01-recipes-db-process.ipynb 85
def parse_ingredient_string(ingredient_string):
    nlp_parse = parse_ingredient_nlp(ingredient_string)
    parsed_ingredient = {
        'name': nlp_parse.name.text if nlp_parse.name else None,
        'quantity': nlp_parse.amount[0].quantity if nlp_parse.amount else None,
        'unit': nlp_parse.amount[0].unit if nlp_parse.amount else None,
        'comment': nlp_parse.comment.text if nlp_parse.comment else None,
        'preparation': nlp_parse.preparation.text if nlp_parse.preparation else None
    }
    return parsed_ingredient

# %% ../../notebooks/01-recipes-db-process.ipynb 94
def pipeline_parse_ingredient_string_parsing(ingredients_df):

    ingredients_df['parsed'] = parallel_apply(
        ingredients_df['full_string'], 
        parse_ingredient_string, 
        meta=pd.Series(dtype='object'), 
        npartitions=500
    )
    
    ingredients_df['parsed'] = ingredients_df['parsed'].apply(literal_eval)

    expanded = pd.json_normalize(ingredients_df['parsed'])
    expanded = expanded.astype('string')
    expanded.set_index(ingredients_df.index, inplace=True)
    ingredients_df = pd.concat([ingredients_df, expanded], axis=1)

    ingredients_df.drop(['parsed'], axis=1, inplace=True, errors='ignore')

    return ingredients_df

# %% ../../notebooks/01-recipes-db-process.ipynb 107
def is_number(s):
    for split in s.split('.'):
        if not split.isdigit():
            return False
    return True

# %% ../../notebooks/01-recipes-db-process.ipynb 109
def clean_quantity(quantity):
    
    if pd.isnull(quantity) or quantity == '': return quantity

    quantity = quantity.lower()
    quantity = quantity.replace('x', '')
    quantity = re.sub(r'\.$', '', quantity)

    if "-" in quantity:
        splits = quantity.split("-")
        if all([is_number(n) for n in splits]):
            quantity = sum([float(n) for n in splits])/len(splits)
    if quantity in ['several', 'few']: 
        quantity = 3.0
    if quantity == 'half': 
        quantity = 0.5

    try:
        quantity = float(quantity)
    except ValueError:
        quantity = pd.NA

    return quantity

# %% ../../notebooks/01-recipes-db-process.ipynb 117
def find_ner_match(ingredient):

    ner_match = pd.NA

    ner_ingredients = ingredient['NER'] if isinstance(ingredient['NER'], np.ndarray) else literal_eval(ingredient['NER'])
    ingredient_index = ingredient.name[1] if isinstance(ingredient.name, tuple) else ingredient['ingredient'] # parallel non-multi index

    if ingredient_index in ner_ingredients:
        search_ingredient = str(ner_ingredients[ingredient_index])
        if search_ingredient in ingredient['name']:
            ner_match = search_ingredient
        elif ingredient['name'] in search_ingredient:
            ner_match = ingredient['name']
    if not pd.notnull(ner_match): # the dataset's NER cuts out some of the ingredients, making the indices not match. In this case a search through the whole NER array is required.
        for search_ingredient in ner_ingredients:
            search_ingredient = str(search_ingredient)
            if search_ingredient in ingredient['name']:
                ner_match = search_ingredient
            elif ingredient['name'] in search_ingredient:
                ner_match = ingredient['name']
            if pd.notnull(ner_match): break
        
    return ner_match

# %% ../../notebooks/01-recipes-db-process.ipynb 172
def tokenize_with_spans(txt):
    tokens=mt.tokenize(txt)
    offset = 0
    for token in tokens:
        offset = txt.find(token, offset)
        yield token, offset, offset+len(token)
        offset += len(token)

# %% ../../notebooks/01-recipes-db-process.ipynb 175
def get_match_idxs(ingredient_string, search_string):
    idx = ingredient_string.find(search_string)
    if idx == -1: 
        return None
    return set(range(idx, idx+len(search_string)))

# %% ../../notebooks/01-recipes-db-process.ipynb 177
def split_nouns(ingredient_string, cut_string):

    if not cut_string: return ([], [])

    # getting tags of full string
    tokens_spanned = list(tokenize_with_spans(ingredient_string))
    tokens_spanned_tags = nltk.pos_tag(list(zip(*tokens_spanned))[0])
    if tokens_spanned_tags: tokens_spanned_tags[-1] = (tokens_spanned_tags[-1][0], 'NN') # setting last word as noun (as this is describing item)
    tokens_spanned_tagged = [word[0] + word[1] for word in list(zip(tokens_spanned, tokens_spanned_tags))] # joining the two together
    tokens_spanned_tagged = list(zip(*[t for i, t in enumerate(list(zip(*tokens_spanned_tagged))) if i != 3])) # removing 3rd index of each tuple in list
    
    # using full string tags to tag cut string
    match_idxs = get_match_idxs(ingredient_string, cut_string)
    if not match_idxs: return pd.NA, pd.NA
    tags = [t for t in tokens_spanned_tagged if match_idxs.intersection(range(*t[1:3]))]
    
    # splitting nouns
    tags_split = [t[0] for t in tags if t[3].startswith('NN')], [t[0] for t in tags if not t[3].startswith('NN')]

    # limit to 6 words for each split
    tags_split = tuple([tags[0:6] for tags in tags_split])

    return tags_split

# %% ../../notebooks/01-recipes-db-process.ipynb 179
def split_ingredient_fields_by_noun(ingredient, debug=False):
    for col in ingredient.index:
        if col == 'ingredient_string': continue
        ingredient[col + '.nouns'], ingredient[col + '.others'] = split_nouns(ingredient['name.description'], ingredient[col])
        if debug:
            if ingredient[col] != '' and (len(ingredient[col + '.nouns']) + len(ingredient[col + '.others']) == 0):
                print('WARN: Missing ingredient tags', ingredient.name, col, ingredient[col], ingredient['ingredient_string'], sep=' | ')
    return ingredient

# %% ../../notebooks/01-recipes-db-process.ipynb 189
def remove_name_from_description(ingredient):
    ingredient_cols = ingredient[ingredient.notnull()].index
    ingredient_name_cols = ingredient_cols[ingredient_cols.str.startswith('name.name')]
    ingredient_desc_cols = ingredient_cols[ingredient_cols.str.startswith('name.description')]
    matching_cols = ingredient[ingredient_desc_cols][ingredient[ingredient_desc_cols].apply(lambda x: x in ingredient[ingredient_name_cols].values)].index
    ingredient[matching_cols] = pd.NA
    return ingredient

# %% ../../notebooks/01-recipes-db-process.ipynb 204
def filter_patterns(ingredient, filters):

    cols = ingredient.index[ingredient.notnull()]

    for filter_words in filters:
        if all([any([filter_word == ingredient[col] for col in cols]) for filter_word in filter_words]):
            return True
        
    return False

filters = [['salt'],['ground', 'pepper'],['black', 'pepper'], ['pepper']]
